<?xml version="1.0" encoding="UTF-8"?>
<!doctype html>
<html>
	<head>
		<style type="text/css">
			h2 {
				font: -apple-system-headline;
			}
			body {
				font: -apple-system-body;
				zoom: 1.2;
			}
			img {
				width: auto;
				height : auto;
				max-height: 100%;
				max-width: 100%;
			}
		</style>
	</head>
	<body>
		<h2>Running an Infinispan server using Testcontainers</h2>
		<p>Recently I discovered a library called <em><a href="https://www.testcontainers.org/">Testcontainers</a></em>. I already <a href="https://reinhard.codes/2017/11/26/mitigating-integration-test-problems-using-testcontainers/">wrote about using it on my current project here</a>. It helps you run software that your application depends on in a test context by providing an API to start docker containers. It’s implemented as a JUnit 4 rule currently, but you can also use it manually with JUnit 5. Native support for JUnit 5 is on the roadmap for the next major release. <em>Testcontainers</em> comes with a few pre-configured database- and selenium-containers, but most importantly it also provides a <a href="https://www.testcontainers.org/usage/generic_containers.html">generic container</a> that you can use to start whatever docker image you need to.</p> 
<p>In my current project we are using <a href="http://infinispan.org/">Infinispan</a> for distributed caching. For some of our integration tests caching is disabled, but others rely on a running Infinispan instance. Up until now we have been using a virtual machine to run Infinispan and other software on developer machines and build servers. The way we are handling this poses a few problems and isolated Infinispan instances would help mitigate these. This post shows how you can get Infinispan running in a generic container. I’ll also try to come up with a useful abstraction that makes running Infinispan as a test container easier.</p> 
<h2>Configuring a generic container for Infinispan</h2> 
<p><a href="https://hub.docker.com/">Docker Hub</a> provides a readymade Infinispan image: <a href="https://hub.docker.com/r/jboss/infinispan-server/"><code>jboss/infinispan-server</code></a>. We’ll be using the latest version at this time, which is <code>9.1.3.Final</code>. Our first attempt to start the server using Testcontainers looks like this:</p> 
<pre> 
@ClassRule 
public static GenericContainer infinispan = 
      new GenericContainer("jboss/infinispan-server:9.1.3.Final"); 
 
@Before 
public void setup(){ 
    cacheManager = new RemoteCacheManager(new ConfigurationBuilder() 
            .addServers(getServerAddress()) 
            .version(ProtocolVersion.PROTOCOL_VERSION_26) 
            .build()); 
} 
   
@Test 
public void should_be_able_to_retrieve_a_cache() { 
    assertNotNull(cacheManager.getCache()); 
} 
 
private String getServerAddress() { 
    return infinispan.getContainerIpAddress() + ":"  
        + infinispan.getMappedPort(11222); 
} 
</pre> 
<p>You can see a few things here:</p> 
<ol><li>We’re configuring our test class with a class rule that will start a generic container. As a parameter, we use the name of the infinispan docker image alongside the required version. You could also use <code>latest</code> here.</li> 
<li>There’s a setup method that creates a <a href="https://docs.jboss.org/infinispan/9.1/apidocs/org/infinispan/client/hotrod/RemoteCacheManager.html"><code>RemoteCacheManager</code></a> to connect to the Infinispan server running inside the docker container. We extract the network address from the generic container and retrieve the container IP address and the mapped port number for the <a href="http://infinispan.org/docs/stable/user_guide/user_guide.html#hot_rod_protocol">hotrod</a> port in <code>getServerAddress()</code></li> 
<li>Then there’s a simple test that will make sure we are able to retrieve an unnamed cache from the server.</li> 
</ol><h3>Waiting for Infinispan</h3> 
<p>If we run the test, it doesn’t work and throws a <code>TransportException</code>, though. It mentions an error code that hints at a connection problem. Looking at other pre-configured containers, we see that they have some kind of waiting strategy in place. This is important so that the test only starts after the container has fully loaded. The <a href="https://github.com/testcontainers/testcontainers-java/blob/1.5.0/modules/postgresql/src/main/java/org/testcontainers/containers/PostgreSQLContainer.java"><code>PostgreSQLContainer</code></a> waits for a log message, for example. There’s other wait strategies available and you can implement your own, as well. One of the default strategies is the <a href="https://github.com/testcontainers/testcontainers-java/blob/master/core/src/main/java/org/testcontainers/containers/wait/HostPortWaitStrategy.java"><code>HostPortWaitStrategy</code></a> and it seems like a straightforward choice. With the Infinispan image at least, it doesn’t work though: one of the commands that is used to determine the readiness of the tcp port <a href="https://github.com/testcontainers/testcontainers-java/pull/524">has a subtle bug in it</a> and the other relies on the <code>netcat</code> command line tool being present in the docker image. We’ll stick to the same approach as the <code>PostgreSQLContainer</code> rule and check for a suitable log message to appear on the container’s output. We can determine a message by manually starting the docker container on the command line using:</p> 
<p><code>docker run -it jboss/infinispan-server:9.1.3.Final</code>.</p> 
<p>The configuration of our rule then changes to this:</p> 
<pre> 
@ClassRule 
public static GenericContainer container = 
    new GenericContainer("jboss/infinispan-server:9.1.3.Final") 
      .waitingFor(new LogMessageWaitStrategy() 
         .withRegEx(".*Infinispan Server.*started in.*\\s")); 
</pre> 
<p>After this change, the test still doesn’t work correctly. But at least it behaves differently: It waits for a considerable amount of time and again throws a <code>TransportException</code> before the test finishes. Since the underlying <code>TcpTransportFactory</code> swallows exceptions on startup and returns a cache object anyway, the test will still be green. Let’s address this first. I don’t see a way to ask the <code>RemoteCacheManager</code> or the <code>RemoteCache</code> about the state of the connection, so my approach here is to work with a timeout:</p> 
<pre> 
private ExecutorService executorService = Executors.newCachedThreadPool(); 
 
@Test 
public void should_be_able_to_retrieve_a_cache() throws Exception { 
    Future&gt; result =  
             executorService.submit(() -&gt; cacheManager.getCache()); 
    assertNotNull(result.get(1500, TimeUnit.MILLISECONDS)); 
} 
</pre> 
<p>The test will now fail should we not be able to retrieve the cache within 1500 milliseconds. Unfortunatly, the resulting <code>TimeoutException</code> will not be linked to the <code>TransportException</code>, though. I’ll take suggestions for how to better write a failing test and leave it at that, for the time being.</p> 
<h3>Running Infinispan in standalone mode</h3> 
<p>Looking at the stacktrace of the <code>TransportException</code> we see the following output:</p> 
<p><code>INFO: ISPN004006: localhost:33086 sent new topology view (id=1, age=0) containing 1 addresses: [172.17.0.2:11222]<br> 
Dez 14, 2017 19:57:43 AM org.infinispan.client.hotrod.impl.transport.tcp.TcpTransportFactory updateTopologyInfo<br> 
INFO: ISPN004014: New server added(172.17.0.2:11222), adding to the pool.</code></p> 
<p>It looks like the server is running in clustered mode and the client gets a new server address to talk to. The IP address and port number seem correct, but looking more closely we notice that the hotrod port <code>11222</code> refers to a port number <b>inside</b> the docker container. It is not reachable from the host. That’s why <em>Testcontainers</em> gives you the ability to easily retrieve port mappings. We already use this in our <code>getServerAddress()</code> method. Infinispan, or rather the hotrod protocol, however is not aware of the docker environment and communicates the internal port to the cluster clients overwriting our initial configurtation.</p> 
<p>To confirm this analysis we can have a look at the output of the server when we start the image manually:</p> 
<p><code>19:12:47,368 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (MSC service thread 1-6) ISPN000078: Starting JGroups channel clustered<br> 
19:12:47,371 INFO  [org.infinispan.CLUSTER] (MSC service thread 1-6) ISPN000094: Received new cluster view for channel cluster: [9621833c0138|0] (1) [9621833c0138]<br> 
...<br> 
Dez 14, 2017 19:12:47,376 AM org.infinispan.client.hotrod.impl.transport.tcp.TcpTransportFactory updateTopologyInfo<br> 
INFO: ISPN004016: Server not in cluster anymore(localhost:33167), removing from the pool.</code></p> 
<p>The server is indeed starting in clustered mode and the documentation on Docker Hub also confirms this. For our tests we need a standalone server though. On the command line we can add a parameter when starting the container (again, we get this from the documentation on Docker Hub):</p> 
<p><code>$ docker run -it jboss/infinispan-server:9.1.3.Final standalone</code></p> 
<p>The output now tells us that Infinispan is no longer running in clustered mode. In order to start Infinispan as a standalone server using <em>Testcontainers</em>, we need to add a command to the container startup. Once more we change the configuration of the container rule:</p> 
<pre> 
@ClassRule 
public static GenericContainer container = 
    new GenericContainer("jboss/infinispan-server:9.1.3.Final") 
      .waitingFor(new LogMessageWaitStrategy() 
         .withRegEx(".*Infinispan Server.*started in.*\\s")) 
      .withCommand("standalone"); 
</pre> 
<p>Now our test now has access to an Infinispan instance running in a container.</p> 
<h2>Adding a specific configuration</h2> 
<p>The applications in our project use different caches, these can be configured in the Infinispan standalone configuration file. For our tests, we need them to be present. One solution is to use the <code>.withClasspathResourceMapping()</code> method to link a configuration file from the (test-)classpath into the container. This configuration file <a href="https://github.com/dnno/testcontainers-infinispan/blob/master/src/test/resources/infinispan-standalone.xml#L167">contains the cache configurations</a>. Knowing the location of the configuration file in the container, we can once again change the testcontainer configuration:</p> 
<pre> 
public static GenericContainer container = 
    new GenericContainer("jboss/infinispan-server:9.1.3.Final") 
      .waitingFor(new LogMessageWaitStrategy() 
         .withRegEx(".*Infinispan Server.*started in.*\\s")) 
      .withCommand("standalone") 
      .withClasspathResourceMapping( 
              "infinispan-standalone.xml", 
              "/opt/jboss/infinispan-server/standalone/configuration/standalone.xml", 
              BindMode.READ_ONLY); 
 
@Test 
public void should_be_able_to_retrieve_a_cache() throws Exception { 
    Future&gt; result =  
         executorService.submit(() -&gt; cacheManager.getCache("testCache")); 
    assertNotNull(result.get(1500, TimeUnit.MILLISECONDS)); 
} 
</pre> 
<p>Now we can retrieve and work with a cache from the Infinispan instance in the container.</p> 
<h2>Simplifying the configuration</h2> 
<p>You can see how it can be a bit of a pain getting an arbitrary docker image to run correctly using a generic container. For Infinispan we now know what we need to configure. But I really don’t want to think of all this every time I need an Infinispan server for a test. However, we can create our own abstraction similar to the <code>PostgreSQLContainer</code>. It contains the configuration bits that we discovered in the first part of this post and since it is an implementation of a <code>GenericContainer</code>, we can also use everything that’s provided by the latter.</p> 
<pre> 
public class InfinispanContainer extends GenericContainer { 
 
  private static final String IMAGE_NAME = "jboss/infinispan-server"; 
 
  public InfinispanContainer() { 
    this(IMAGE_NAME + ":latest"); 
  } 
 
  public InfinispanContainer(final String imageName) { 
    super(imageName); 
    withStartupTimeout(Duration.ofMillis(20000)); 
    withCommand("standalone"); 
    waitingFor(new LogMessageWaitStrategy().withRegEx(".*Infinispan Server.*started in.*\\s")); 
  } 
 
} 
</pre> 
<p>In our tests we can now create an Infinispan container like this:</p> 
<pre> 
@ClassRule 
public static InfinispanContainer infinispan = new InfinispanContainer(); 
</pre> 
<p>That’s a lot better than dealing with a generic container. </p> 
<h3>Adding easy cache configuration</h3> 
<p>You may have noticed that I left out the custom configuration part here. We can do better by providing builder methods to create caches programatically using the <code>RemoteCacheManager</code>. Creating a cache is as easy as this:</p> 
<pre> 
cacheManager.administration().createCache("someCache", null); 
</pre> 
<p>In order to let the container automatically create caches we facilitate the callback method <code>containerIsStarted()</code>. We can overload it in our abstraction, create a <code>RemoteCacheManager</code> and use its API to create caches that we configure upfront:</p> 
<pre> 
... 
private RemoteCacheManager cacheManager; 
private Collection cacheNames; 
... 
 
public InfinispanContainer withCaches(final Collection cacheNames) { 
    this.cacheNames = cacheNames; 
    return this; 
} 
 
@Override 
protected void containerIsStarted(final InspectContainerResponse containerInfo) { 
    cacheManager = new RemoteCacheManager(new ConfigurationBuilder() 
        .addServers(getServerAddress()) 
        .version(getProtocolVersion()) 
        .build()); 
 
    this.cacheNames.forEach(cacheName -&gt;  
        cacheManager.administration().createCache(cacheName, null)); 
} 
 
public RemoteCacheManager getCacheManager() { 
    return cacheManager; 
} 
</pre> 
<p>You can also retrieve the <code>CacheManager</code> from the container and use it in your tests.<br> 
There’s also a problem with this approach: you can only create caches through the API if you use Hotrod protocol version 2.0 or above. I’m willing to accept that as it makes the usage in test really comfortable:</p> 
<pre> 
@ClassRule 
public static InfinispanContainer infinispan = 
      new InfinispanContainer() 
          .withProtocolVersion(ProtocolVersion.PROTOCOL_VERSION_21) 
          .withCaches("testCache"); 
 
@Test 
public void should_get_existing_cache() { 
    assertNotNull(infinispan.getCacheManager().getCache("testCache")); 
} 
</pre> 
<p>If you need to work with a protocol version below 2.0, you can still use the approach from above, linking a configuration file into the container.</p> 
<h2>Conclusion</h2> 
<p>While it sounds very easy to run any docker image using <em>Testcontainers</em>, there’s a lot of configuration details to know, depending on the complexity of the software that you need to run. In order to effectivly work with such a container, it’s a good idea to encapsulate this in your own specific container. Ideally, these containers will end up in the <em>Testcontainers</em> repository and others can benefit of your work as well.<br> 
I hope this will be useful for others, if you want to see the full code, <a href="https://github.com/dnno/testcontainers-infinispan">have a look at this repository</a>. </p> 
<p>The post <a href="https://blog.codecentric.de/en/2017/12/running-infinispan-server-using-testcontainers/">Running an Infinispan server using Testcontainers</a> appeared first on <a href="https://blog.codecentric.de/en">codecentric AG Blog</a>.</p> 
<img src="http://feeds.feedburner.com/~r/codecentric_en/~4/y76QhQ0gqOk" height="1" width="1" alt="">
	</body>
</html>
